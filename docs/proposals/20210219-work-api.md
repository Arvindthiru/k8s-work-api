---
title: Proposal Template
authors:
  - "@qiujian16"
reviewers:
  - "@pmorie"
  - "@jeremyot"
creation-date: 2021-02-19
last-updated: 2021-02-19
status: provisional
---

# Work API

## Summary

This proposal is for a minimalistic start to a new `Work` API which is intended to group a set of k8s API resources to be applied to one remote cluster together as a concept of `work` or `workload`.  It could be simple enough that it does not relate to cluster registration mechanisms or any workload scheduling on multiple clusters.

## Motivation

There are already several different techniques to distribute workload on multiple kubernetes clusters, including `kubefed v1`, `kubefed v2`, and `gitops`. All these techniques has some common patterns for the function to deploy workload resource manifests to one or multiple clusters.

1. A single source of truth which could be git, cloud storage, kube-apiserver or rpc server.
2. A control loop to apply resources manifests from a single source of truth to a cluster, return apply results and track resource status.
3. A way to decide which clusters the resource manifests should apply to.
    - Specific levels of redundancy.
    - Specific kinds of geographic/topological placement or spread.
    - Specific resource availability.

In addition, the control loop will also face the problem of garbage collecting resources when there is no intent to deploy them on a cluster.

All of the above motivates the notion of work api which:

1. Allow developers to easily integrate with other sources of truth, e.g. git, another kube-apiserver. And the control loop of work api could provide a generic way to apply resource manifests to a certain cluster.
2. Easy to integrate with other placement primitives.
3. Track which clusters a particular workload is deployed to.
4. Track the deployed resource on a cluster so the control loop could garbage collect these resources.

### Goal

- API Specification of Work.
- A reference implementation of contoll loops for Work API.

### Non-goal

- How workload is scheduled among multiple clusters.

## Proposals

### Terminology

- Work Hub: A place that work API resides. It could be a k8s cluster playing the role as the management plane for other k8s clusters. Or it could also be an RPC server or a cloud API depending on the detailed implementation. Users create work API resources on the work hub. In the rest of the doc, we assume the implementation of work hub is based on k8s cluster for simplicity.
- Managed Cluster: A k8s cluster managed by the work hub. The resources defined in the work API are applied on the managed cluster. It was also referred to “Spoke” or “Spoke Cluster”, though we use “Managed Cluster” in this document.
- Work Controller: a controller that reconciles the work object on hub, and applies resources defined in work to the managed cluster.

### User Stories
#### Deploy a workload to a managed cluster
I have 3 clusters. One is the work hub, the other two are managed clusters. I want to declare the workload (deployment/configmap/service etc) on the work hub, and ensure the workload will be deployed on the desired managed cluster. I want to ensure that when I update the workload declaration, the real deployed workload on the managed  cluster reflects the update.

#### Track the status of workload in another cluster
After I have declared the workload on the work hub, I want to track whether the workload has been successfully deployed on the managed cluster, and the workload is running normally.

#### Garbage collect the workload
When I do not want to deploy the workload on a managed cluster, I can clean the workload by deleting the api on the work hub. 

### Proposal Details

We propose a new CRD called `Work` to represent a list of api resources to be deployed on a cluster. `Work` is created on the work hub, and resides in the namespace that the work controller is authorized to access. Creation of a `Work` on the work hub indicates that resources defined in `Work` will be applied on a certain managed cluster. Update of a `Work` will trigger the resource update on the managed cluster, and deletion of a `Work` will recycle the resources on the managed cluster.

If there are multiple managed clusters, multiple work controllers will be running that monitor the `Work` API in same or different namespaces in the work hub. It is possible that multiple work controllers watch `Works` in one namespace on a work hub and deploy the resources on multiple clusters. It is also possible that multiple work controllers watch `Works` in different namespaces on the work hub, so a Work created in one namespace triggers the resource deployment on a certain cluster.

#### Resources in the work

The resources in kubernetes could be classified to several categories:
- Workload related resources: deployments/statefulset, configmaps, ns-scoped custom resources etc.
- Clusterwide configuration resources: apiservices, CRDs, storageclasses
- Credentials: secrets

`Work` api should be mainly used for workload related resources.
Secrets should not be declared in `work` apis, other techniques (e.g. vault) should be considered as a more secure way to transmit secrets among clusters. 

#### Push/Pull Model

There have been discussions in the community on whether the workload distribution in multicluster should use a push or pull model. 

Push model means that a controller on the hub watches APIs defining workload and “PUSH” the resource manifest to the managed cluster. There are some limitations in the push model:

- It requires apiserver of each managed cluster must be accessed by the work hub where the controller is running. This could be a hard requirement since some managed clusters may hide behind firewalls and do not have a public accessible IP. Exposing apiserver of all the managed clusters also enlarges the surface to be attacked.
- It requires credentials of managed clusters with sufficient permission to be put on the work hub. The credential has to be passed in an out of band secure way. 
- Having a centralized controller to distribute workload to many clusters could have scalability limitations.

Pull model means that an agent running in the managed cluster watches APIs defined on hub, fetches them and applies locally on the managed cluster. Compared to the push model, the API exposure surface is reduced since only apiserver of the work hub needs to be publicly accessible. The credential for the agent to talk to the work hub can have very limited permission only on certain APIs. 

`Work` API itself is not constrained to be used only on push or pull mode. The work controller could reside in a managed cluster that “PULLS” the API and applies locally on the managed cluster. It  could also reside in the work hub that watches the Wok API and “PUSHES” the workload to the managed cluster. 

#### Working with higher primitive

`Work` represents a workload to be deployed in a target namespace on a single managed cluster. Which cluster the work is to be deployed and how the `Work` is scheduled to a certain cluster is not defined in the `Work` API. A higher primitive could be used to generate `Work` based on a scheduling decision and place the workload on a managed cluster. The higher primitive must coordinate with clusterset together to decide which clusters the `Work` should place to.

#### API Specification

```go
// Work defines a list of resources to be deployed on the managed cluster
type Work struct {
   metav1.TypeMeta   `json:",inline"`
   metav1.ObjectMeta `json:"metadata,omitempty"`

   Spec   WorkSpec   `json:"spec,omitempty"`
   Status WorkStatus `json:"status,omitempty"`
}

// WorkSpec defines the desired state of Work
type WorkSpec struct {
   // Workload represents the manifest workload to be deployed on managed cluster
   Workload WorkloadTemplate `json:"workload,omitempty"`
}

// WorkloadTemplate represents the manifest workload to be deployed on managed cluster
type WorkloadTemplate struct {
   // Manifests represents a list of kuberenetes resources to be deployed on the managed cluster.
   // +optional
   Manifests []Manifest `json:"manifests,omitempty"`
}

// Manifest represents a resource to be deployed on managed cluster
type Manifest struct {
   runtime.RawExtension `json:",inline"`
}

// WorkStatus defines the observed state of Work
type WorkStatus struct {
   // Conditions contain the different condition statuses for this work.
   // Valid condition types are:
   // 1. Applied represents workload in Work is applied successfully on a managed cluster.
   // 2. Progressing represents workload in Work is being applied on a managed cluster.
   // 3. Available represents workload in Work exists on the managed cluster.
   // 4. Degraded represents the current state of workload does not match the desired
   // state for a certain period.
   Conditions []metav1.Condition `json:"conditions"`

   // ManifestConditions represents the conditions of each resource in work deployed on
   // managed cluster.
   // +optional
   ManifestConditions []ManifestCondition `json:"manifestConditions,omitempty"`
}

// ResourceIdentifier provides the identifiers needed to interact with any arbitrary object.
type ResourceIdentifier struct {
   // Ordinal represents an index in manifests list, so the condition can still be linked
   // to a manifest even though manifest cannot be parsed successfully.
   Ordinal int `json:"ordinal,omitempty"`

   // Group is the group of the resource.
   Group string `json:"group,omitempty"`

   // Version is the version of the resource.
   Version string `json:"version,omitempty"`

   // Kind is the kind of the resource.
   Kind string `json:"kind,omitempty"`

   // Resource is the resource type of the resource
   Resource string `json:"resource,omitempty"`

   // Namespace is the namespace of the resource, the resource is cluster scoped if the value
   // is empty
   Namespace string `json:"namespace,omitempty"`

   // Name is the name of the resource
   Name string `json:"name,omitempty"`
}

// ManifestCondition represents the conditions of the resources deployed on
// managed cluster
type ManifestCondition struct {
   // resourceId represents the identity of a resource linking to manifests in spec.
   // +required
   Identifier ResourceIdentifier `json:"identifier,omitempty"`

   // Conditions represents the conditions of this resource on the managed cluster
   // +required
   Conditions []metav1.StatusCondition `json:"conditions"`
}
```

An example of the `Work` API looks like:

```yaml
apiVersion: multicluster.x-k8s.io/v1alpha1
kind: Work
metadata:
 name: work-sample
 namespace: cluster
spec:
 workload:
   manifests:
   - apiVersion: v1
     kind: ConfigMap
     metadata:
       name: cm
       namespace: default
     data:
       ui.properties: |
         color=purple
```
which declares the intention to apply a configmap to a certain managedcluster.

#### Conditions

Manifest conditions represent the status conditions of a certain manifest to be applied on a managed cluster. The structure of manifest conditions will include an identifier to link to a resources defined in work.spec field, and a list of conditions showing the current status of the resource being applied. Condition types for manifest conditions inludes:
- *Applied* indicates that the manifest with the identifier is applied successfully in the managed cluster.
- *Available* indicates that the resource relating to the manifest exists in the managed cluster.
- *Degraded* indicates that the manifest applied on the managed cluster does not match the desired status. Example is the running replica in deployment does not fit the desired replica in deployment spec.

In addition to track status of each manifest with manifest condition, a `Work` should have summarized conditions based on manifest conditions. An example of `Work` and manifest conditions together as below:

```yaml
conditions:
- lastTransitionTime: "2020-07-02T03:16:26Z"
  message: Apply manifest work complete
  reason: AppliedManifestWorkComplete
  status: "True"
  type: Applied
manifestConditions:
- conditions:
  - lastTransitionTime: "2020-07-02T03:16:26Z"
    message: Apply manifest complete
    reason: AppliedManifestComplete
    status: "True"
    type: Applied
  identifier:
    group: ""
    kind: ConfigMap
    name: cm1
    namespace: default
    ordinal: 0
    resource: configmaps
    version: v1
```

### Test Plan

E2E tests can use kind to create multiple clusters to test.

- Creating a work resource on hub will trigger the resource creation on a managedcluster
- Updating a work resource on hub will trigger the resource update on a managedcluster
- Deleting a work resource on hub will trigger the resource deletion on a managedcluster

### Graduation Criteria

#### Alpha -> Beta Graduation

- E2E tests exists
- Beta -> GA Graduation criteria defined.
- At least one reference implementation using controller.

#### Beta -> GA Graduation

- Scalability/performance testing.